{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用YOLO,YOLOv2和最新的YOLOv3做实时物体检测\n",
    "【翻译自】：https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088\n",
    "**You only look once(YOLO)** 是一个用于实时物体识别的系统。在本文中我们将介绍YOLO，YOLOv2以及YOLO9000。对于只关心YOLOv3的人，可以直接调准到文章最后。下图展示的是YOLO网站提供的准确率和速度比较。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*ju1oaoIkVUkaIPAdpehlzA.png)\n",
    "让我们从下图开始。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*EYFejGUjvjPcc4PZTwoufw.jpeg)\n",
    "YOLO的目标检测结果如下：<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*QOGcvHbrDZiCqTG6THIQ_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网格单元\n",
    "为了便于讨论，我们对原始图像进行裁剪。YOLO将输入图像分割为SxS个网格。**每个网格只预测一个物体**。例如，下图中黄色的网格尝试预测中心（蓝色点）落入网格单元的“人”这一类物体。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg)\n",
    "每个网格单元预测固定数量的边界框。在本例中，黄色的网格预测了两个边界框（蓝色框）来定位“人”。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg)\n",
    "然而，“只预测一个物体”这个规则却限制了识别物体的精度。因此，YOLO在精度上有一些欠缺。对于下图，YOLO只检测出了9个老人中的5个。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*j4PnWfxP3yoVPOFyI27tww.jpeg)\n",
    "对于每个单元格来说，<br>\n",
    "- 会预测B个边界框，每个框有一个置信得分\n",
    "- 用B个边界框也只是检测一个物体\n",
    "- 预测C个类别概率（C对应了物体的类别）\n",
    "为了在PASCAL VOC上验证，YOLO使用了7x7的网格（SxS），2个边界框（B）以及20个类（C）。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*OuMJUWo2rXYA-GYU63NUGw.jpeg)\n",
    "我们继续来看更多细节。每个边界框包含5个元素：(x,y,w,h)以及一个置信得分。这个置信得分反应的是框内包含物体的概率，以及边界框的准确率。我们边界框的宽度w和高度h按照图像的宽度和高度做标准化。x和y是对应单元格的偏移量。然而，x,y,w以及h都是0到1之间的数。每个单元包含20个条件化的类别概率。**条件化的类别概率**指的是检测到的物体归属于一个特定类别的概率（每个单元格上每个类别对应一个概率）。所以，YOLO的预测结果的尺寸是(S,S,Bx5+C)=(7,7,2x5+20)=(7,7,30)。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg)\n",
    "YOLO主要的原理就是构建一个CNN网络来预测一个(7,7,30)的张量。他使用CNN网络将每个位置上的空间维度降到7x7x1024的维度。YOLO使用两个全连接层来做线性回归，得到了7x7x2个边界框预测结果（下面中间的图）。为了最终的预测，我们保留那些高置信得分（大于0.25）的框作为最终的预测结果（下面右图）。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*e0VY6U1_WMF2KBoKQNZvkQ.png)\n",
    "每个预测框的置信得分计算如下：<br>\n",
    "$$class\\ confidence\\ score=box\\ confidence\\ score \\times conditional\\ class\\ probability$$\n",
    "这个指标可以衡量分类和定位结果的置信度（一旦物体被检测到）。<br>\n",
    "我们可以将以上得分和概率项轻易地汇总起来。下面是参考文献中数学定义。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*0IPktA65WxOBfP_ULQWcmw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络设计\n",
    "![avater](https://cdn-images-1.medium.com/max/1000/1*9ER4GVUtQGVA2Y0skC9OQQ.png)\n",
    "YOLO包含24个卷积层，以及两个全连接层。一些卷积层使用了1x1卷积层来减少特征图的深度。对于最后一个卷积层，它的输出张量是(7,7,1024)的。然后整个张量被扁平化。使用两个全连接层用作现行回归，它输出7x7x30个参数，然后重塑为(7,7,30)，即每个位置有两个边界框预测值。<br>\n",
    "一个速度较快但是精度较低版本的YOLO，叫做Fast YOLO，只使用使用9个卷积层。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "YOLO在每个单元格上预测多个边界框。为了计算相对于真值的损失，我们其实只需要一个检测到物体的边界框。为了达到这个目的，我们选择相对于真实值IoU（交并比）最高的那个。这使得最终的预测结果在特定的尺寸和纵横比上都达到了更好的效果。<br>\n",
    "YOLO在预测结果和真实值之间使用平方和误差计算损失。损失函数包含如下：<br>\n",
    "- 分类损失\n",
    "- 定位损失（预测边界框和真实值的误差）\n",
    "- 置信损失（边界框是否包含物体）\n",
    "\n",
    "### 分类损失\n",
    "如果检测到物体，每个单元格的分类损失就是每个类别概率之差的平方和：<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*lF6SCAVj5jMwLxs39SCogw.png)\n",
    "\n",
    "### 定位损失\n",
    "定位损失用于衡量预测边界框的位置和尺寸。我们只计算有物体边界框的损失。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*BwhGMvffFfqtND9413oiwA.png)\n",
    "我们对不同尺寸大小框的绝对误差一视同仁，也就是说较大边框和较小边框的两像素误差是一样的。为了强调这一点，YOLO预测的是边界框长度和宽度的平方根的差之和。另外，为了强调边界框准确率的重要性，我们会在损失的基础上乘以一个$\\lambda coord$（默认值是5）。\n",
    "\n",
    "### 置信损失\n",
    "如果框中检测到了物体，置信损失的表达式如下:<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*QT7mwEbyLJYIxTYtOWClFQ.png)\n",
    "如果框中没有检测到物体，置信损失的表达式如下：<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*Yc_OJIXOoV2WaGQ6PqhTXA.png)\n",
    "大多数的边界框都不包含物体。这会导致类别不平衡问题，也就是说我们训练模型检测到更多的是背景而不是物体。为了缓解这种情况，我们通过系数$\\lambda nobj$（默认值0.5）对损失加权。\n",
    "\n",
    "### 损失\n",
    "最终的损失是把定位损失，置信损失和分类损失加起来。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*aW6htqx4Q7APLrSQg2eWDw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推断：非极大值抑制(NMS)\n",
    "YOLO会对同一个物体多次预测。为了修正这种情况，YOLO使用了NMS来去除低置信度的冗余预测。NMS会带来2-3%的mAP提升。<br>\n",
    "这里是一种可能的NMS实现:<br>\n",
    "1. 对预测结果按照置信得分排序。\n",
    "2. 从最高的得分开始，无论位置，如果找到之前的预测跟当前预测有同样的类别，但是当前预测的IoU>0.5，就忽略掉该预测。\n",
    "3. 重复第二步直到所有的预测结果被检查到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO的好处\n",
    "- 快，支持实时处理。\n",
    "- 预测结果（物体位置和分类）是单一网络得到的，可以进行端到端地训练来提升准确率。\n",
    "- YOLO的泛化能力较强。当从识别自然图像到其他领域如艺术图像时，YOLO的表现更好。\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*XbOnbcZmc50hyhhTwhD5QA.png)\\\n",
    "- 候选区域方法将分类器限制在了特定的区域。YOLO在整张图像上做预测。由于更多的上下文，YOLO可以在背景区域得到更少的假正例。\n",
    "- YOLO每个单元格只检测一个物体。这使得在预测时会又更多的空间多样性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv2\n",
    "SSD是YOLO的一个有力的竞争对手，其在实时处理的基础上可以得到更高的准确率。相较于基于区域的检测器，YOLO定位方面的误差更高，召回率（定位所有物体结果的评价指标）更低。YOLOv2是YOLO的第二个版本，目标就是提高处理速度的同时提高准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度提升\n",
    "### 批规范化\n",
    "在卷积层加入批规范化可以省去Dropout，是的mAP得以提升2%。\n",
    "### 高分辨率分类器\n",
    "YOLO训练包含两个步骤。首先，我们训练一个如VGG16一样的网络。然后我们将全连接层替换为卷积层并且重新进行端到端的训练。YOLO使用224x224的图片来训练分类器，紧跟448x448的图片来做物体检测。YOLOv2从224x224的图片开始训练分类器然后用448x448的图片以更少的代数微调分类器。这使得检测器在训练时更容易收敛而且使得mAP提升4%。<br>\n",
    "### 利用锚定边界框做卷积\n",
    "如YOLO论文中提到的，早期的训练中对于不稳定的梯度是非常敏感的。因此初始情况下，YOLO会随机地假设一些边界框。这些假设对于一些物体的识别帮助很大，但是另外一些可能表现的不好，这会导致阶跃梯度变化。在早期训练时，预测结果往往会纠结选择什么样的形状进行匹配。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*7iwTsezrn-tSndx96twprA.jpeg)\n",
    "在真实世界中，边界框并不是任意的。汽车有相似的形状，行人的横纵比都接近于0.41。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*krGqonOLMzSE_PWqH_LvQA.jpeg)\n",
    "由于我们只需要一个正确的假设，因此如果我们在初始训练时选择差异性较大的，贴近于真实世界的边界框，训练过程会更加稳定。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*CGWTPTY0sfvQoxsS0X6VFg.jpeg)\n",
    "例如，我们可以创建如下所示的5个候选框。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*8Q8r9ixjTiKLi1mrF36xCw.jpeg)\n",
    "不同于选择任意的5个边界框，我们预测相对于如上图中5个边界框的偏移量。如果我们对偏移量的值做约束，我们可以保持预测结果的多样性并且使得每个预测关注在一个特定的形状。所以初始的训练过程会更稳定。<br>\n",
    "在文章中，锚点也叫作**“先验”**。<br>\n",
    "下面是我们对网络做出的改进:<br>\n",
    "- 去掉用于预测边界框的全连接层。\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*tavjieD0Bum_uX-svYUTKA.jpeg)\n",
    "- 我们将类别预测从单元格级别改变为边界框级别。现在，每个预测包含边界框的4个参数，1个置信得分（是否包含对象）以及20个类别概率。也就是5个边界框每个包含25个参数，即每个单元格是125个参数。同YOLO一样，对象性预测仍然是预测真实值和候选边界框的IOU。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*UsqjfoW3sLkmyXKQ0Hyo8A.png)\n",
    "- 为了生成7x7x125的预测结果，我们需要将最后一个卷积层替换为3x3x1024的卷积层。然后应用1x1卷积层将7x7x1024转换为7x7x125（详细可以参考DarkNet）。\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*KiSd2CBfcs5af6oliHCqzw.jpeg)\n",
    "- 将输入图像尺寸从448x448变更为416x416。这会导致一个奇数的空间尺寸(7x7 vs.8x8单元格)。一张图片的中心通常是由一个较大的物体占据的。如果是奇数单元格，可以更精确地定位物体。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*89qezEeLKJLpD8_fM_H4qQ.jpeg)\n",
    "- 去掉一个池化层使得网络的输出为13x13，而不是7x7。<br>\n",
    "锚定边界框使得mAP从69.5微微减小到了69.2，但是召回率从81%提高到了88%，即精度微微下降但是增加了检测到所有真实物体的机会。<br>\n",
    "### 维度聚类\n",
    "在很多领域中，边界框有很强的模式。例如，在自动驾驶中，两个最常见的边界框是不同距离上的车和行人。为了检测覆盖训练集最好的K个边界框，我们可以利用K-means在训练数据上做聚类，得到最好的K个聚类的中心。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*esSmI0UaMr-GrqkUGMg0hA.jpeg)\n",
    "由于我们处理的是边界框而不是点，我们不能使用常规的空间距离来衡量数据点的距离。毫无疑问，我们使用IoU。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*l5wvrPjLlFp6Whgy0MqbKQ.png)\n",
    "在左图中，我们绘制的不同的聚类数（锚点数）下锚点和真实值的平均IoU。随着锚点增加，精度平稳提升。为了得到最佳表现，YOLO选择了5个锚点。在右图中，是5个锚点的形状。紫蓝色的矩形是从COCO数据集中选择的，而黑色边框的矩形是从VOC2007数据集中选择的。这两种类型的边界框都是更加瘦高以贴近真实世界的边界框。<br>\n",
    "下文中只要我们不对YOLO和YOLOv2对比，我们说YOLO其实就是说的YOLOv2。<br>\n",
    "### 直接位置预测\n",
    "我们预测的是相对于锚点的偏移量，但是如果我们不加限制的话，预测结果仍是随机的。YOLO预测了5个参数（tx,ty,tw,th,to）,并且应用sigma函数来限制便宜的范围。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*J4B5ME3ZXH_GGgVvSSkuBQ.png)\n",
    "下面是以上公式的可视化。下图中的蓝色框是预测边界框，虚点矩形是锚点。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg)\n",
    "基于k-means聚类（维度聚类）以及本节中的提升方法，mAP可以提高5%。<br>\n",
    "\n",
    "### 细粒度特征\n",
    "卷积层一步步地降低了空间的维度。随着解析度下降，检测小物体变得越来越难。其他的物体检测器如SSD可以定位不同特征图上的物体。所以每层对应一个不同的尺度。YOLO采用另一种叫做直通的方法。它将28x28x512的层转换为14x14x2048的层。然后它跟原始的14x14x1024输出层整合，然后在这个整合后的14x14x3072的输出层应用卷积核做预测。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*3fLdTcbKEeeQAt4mt9LnFQ.jpeg)\n",
    "\n",
    "### 多尺度训练\n",
    "在去除了全连接层之后，YOLO可以接收不同尺寸的输入图像。如果宽度和高度是两倍的尺寸，我们我们将得到4倍的输出网格以及4倍的预测结果数。由于YOLO对输入降采样的倍数是32，我们只需要确保宽度和高度是32的倍数即可。在训练时，YOLO接收到的输入图像是320x320,352x352,...以及608x608（步长是32）。对于每10个批次，YOLOv2随机选择不同的尺寸来训练模型。这其实可以看作一种数据增强，使得模型能够更好地预测不同尺寸和维度的图像。另外，我们可以使用较低解析度的图像用于物体检测，这可能会损失一些精度，但是会带来速度的提升。在288x288分辨率下，YOLO可以达到90FPS的同时达到跟Fast R-CNN媲美的mAP。高分辨率YOLO在VOC2007上获得了78.6的mAP。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度\n",
    "这里是应用不同技术所对应的精度提升比较：<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*3IdCKSzR5R0lIE1LSmN4Bg.png)\n",
    "不同检测器的精度比较：<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*NJj17Z6FgffYaA4WH2WIjw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 速度提升\n",
    "### GoogLeNet\n",
    "对一个224x224的图像做一次前向传递，VGG16需要306.9亿次浮点运算，而一个改进的GoogLeNet，只需要85.2亿次。我们可以将VGG16替换为改进的GoogLeNet。然而，YOLO在ImageNet上的top5准确率从90.0%下降到了88.0%。\n",
    "### DarkNet\n",
    "我们可以进一步简化骨干网络。Darknet只需要55.8亿次运算。利用DarkNet，YOLO在ImageNet达到了72.9%的top1准确率和91.2%的top5准确率。Darknet使用3x3的滤波器来提取特征，而用1x1的滤波器来降维。它同时使用全局平均池化来做预测。下面是网络的详细描述：<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*8FiQUakp9i4MneU4VXk4Ww.png)\n",
    "我们将最后一层卷积层替换为三个3x3的卷积层，每个卷积输出通道是1024。然后我们应用1x1卷积层将7x7x1024的输出转换为7x7x125的输出。（5个边界框，每个包含4个位置一个置信分以及20个类别概率，5x(4+1+20)）\n",
    "![avater](https://cdn-images-1.medium.com/max/1000/1*NBnDpz8fitkhcdnkgF2bvg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "YOLO在ImageNet分类数据集上训练了160代：使用随机梯度下降，学习率为0.1，多项式学习率衰减为4的幂，权重衰减为0.0005，动量为0.9。在初始训练时，YOLO使用224x224的图像，然后调优时使用448x448的图像，以10-3的学习率训练10代。在训练后，分类器可以达到76.5%的top1准确率以及93.3%的top5准确率。<br>\n",
    "全连接层和最后的卷积层被移除。YOLO加上了三个3x3的卷积层，每个包含1024的通道，之后跟着一个1x1的卷积层包含125个通道。（5个框每个框25个参数）YOLO同时增加了一个直通层。YOLO在整个网络上以10-3的学习率训练了160代，然后在60代和90代的时候学习率降为之前的10分之一。YOLO使用0.0005的权重衰减和0.9的动量。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类\n",
    "物体检测数据集相较于分类问题的类别更少。为了增加YOLO能够检测的类别，YOLO提出了一个方法在训练时整合检测和分类数据集。它可以利用物体检测样本进行端到端的训练，同时对分类样本的损失做反向传播来训练分类器。这种方法有一些挑战：\n",
    "- 如何整合不同数据集的标签？特别地，物体检测数据集和不同的分类数据集使用不同的标签。\n",
    "- 任何混合的标签可能不是相互排斥的，比如，Imagenet中的Norfolk terrier和COCO中的dog。由于它们不是互相排斥的，我们不能使用softmax来计算概率。\n",
    "\n",
    "### 分级分类\n",
    "简单来讲，YOLO将不同的数据集标签整合为一个树形结构**WordTree**。所有的叶子构成了一个从属关系，比如大飞机是一个飞机。但是整合后的标签将不互相排斥。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*QKwSclDLcT8eJxyj3CVL0w.png)\n",
    "让我们使用1000类的ImageNet数据集来简化问题。不同于利用一种扁平的结构来预测1000个标签，我们创建了一个包含1000个叶子节点（原始标签）和369个父节点的WordTree。利用WordTree，在预测大飞机时，给出的是首先为飞机的得分。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*TALo1-LuWbF80RTCM7bszw.png)\n",
    "有<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*0Oyd6FWXo7OJzzuXuXSoDQ.png)\n",
    "我们可以应用一个softmax函数来计算概率，根据它自己和它的旁系。区别在于，不同于softmax运算，YOLO对于每一个父节点的子节点应用softmax。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*_gX42bmgvyib4lyR9GQrAg.png)\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg)\n",
    "类别概率然后从YOLO预测结果得到。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*0XHKAh0GxpW22y1DlL4hYA.png)\n",
    "对于分类，我们假定物体已经检测到，而Pr(physical object)=1。<br>\n",
    "分级分类的一个好处是当YOLO不能区分飞机的类型，它会给飞机一个很高的分数而不是将其归为一个子类。<br>\n",
    "当YOLO看到一个分类图像，它仅仅用分类损失来训练分类器。YOLO学习某一个类别预测概率最高的边界框并且计算分类损失。（如果物体被标记为大飞机，同时会考虑它标记为飞机，空气，机车...的情况）这驱使模型提取一些通用的特征。所以尽管我们从没有训练一个特定类别的物体检测，我们仍然通过预测相关物体来辅助预测。<br>\n",
    "在物体检测中，我们设置Pr(physical object)等于框的置信得分，这个得分用于标识框中是否有物体。YOLO遍历树，在每个分支选择最高得分的路径直到达到阈值，YOLO最终采用这个类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO9000\n",
    "YOLO9000对YOLO进行扩展，通过分级分类以及WordTree可以检测超过9000类的物体。它整合了COCO的样本以及ImageNet的9000个类别。YOLO对于每一个COCO数据选择4个ImageNet数据。它根据COCO检测数据来检测物体而根据ImageNet样本来分类物体。<br>\n",
    "在评估过程中，YOLO在可以分类的种类上测试图像，也就是不在COCO数据集中的图像来分类。YOLO9000根据包含200个类别的ImageNet检测数据集评估结果。这个数据集跟COCO共享44个种类。因此，数据集一共包含156个不用于定位的类别。YOLO检测提取相关物体类型的特征。因此，我们可以简单地根据这些特征检测这156个类别。<br>\n",
    "YOLO9000在这156个类别上得到了19.7的mAP。YOLO9000在COCO上没有见过的动物种类上表现的很好，因为它们的形状可以轻易地从它们的父类泛化过去。然而，COCO没有边框的标签，所以测试时对于像“太阳镜”的种类就很纠结。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv3\n",
    "利用一个Pascal Titan X，YOLOv3可以在COCO测试集达到30FPS的同时保持57.9%的mAP。<br>\n",
    "### 类别预测\n",
    "大多数的分类器假定输出类别是互相排斥的。然而，YOLO应用了一个softmax函数来将得分转换为概率。YOLOv3使用多标签分类。例如，输出标签可能是“行人”以及“孩子”，这两个类别并不是互相排斥的。（输出的和加总可能大于1）YOLOv3将softmax函数替换为独立的逻辑分类器来计算输入归属于某一个标签的可能性。不同于使用均方差来计算分类损失，YOLOv3使用二元交叉熵损失来计算每个标签。通过避免softmax函数还可以降低计算复杂度。<br>\n",
    "### 边界框预测 & 损失函数计算\n",
    "YOLOv3使用逻辑回归来预测每个边界框的置信得分。YOLOv3改变了计算损失函数的方法。如果边界框先验（锚点）比别的框覆盖了更多的真实值，相应的置信得分就是1。对于其他远远大于预定义阈值（默认0.5）的先验，不纳入损失。每个真实值跟只跟一个边界框先验相关。如果一个边界框先验没有指定，不参与计算分类损失和定位损失，仅计算置信损失。我们使用tx和ty（而不是bx和by）来计算损失。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*UuIx6sH39tefFMxgWAaaNQ.png)\n",
    "\n",
    "### 特征金字塔网络（FPN）\n",
    "YOLOv3在每个位置预测3个结果。每个预测包含一个边界框，一个置信得分，以及80个类别得分，即NxNx[3x(4+1+80)]个预测。<br>\n",
    "YOLOv3在三种尺度上进行预测（类似于FPN）:<br>\n",
    "1. 在最后一个特征图层\n",
    "2. 回退两层进行两倍上采样。YOLOv3取一个高分辨率的特征图并将其合并到未上采样的特征图，合并的方法是元素级的相加。YOLOv3在合并后的特征图上做卷积得到第二个预测结果集。\n",
    "3. 重复第二步使得结果特征图有较好的高层结构（语义）信息，并且有更高的分辨率可以用于物体定位。\n",
    "\n",
    "为了选择先验，YOLOv3进行了k-means聚类。然后预选择9个聚类。对于COCO，先验的宽和高分别是(10x13),(16x30),(33x23),(30x61),(62x45),(59x119),(116x90),(156x198),(373x326)。这9个先验根据尺度归为三种不同的组。每个组指定一个特定的特征图。\n",
    "\n",
    "### 特征提取器\n",
    "一个新的53层的Darknet-53替换了Darknet-19来做为特征提取器。Darknet-53主要包含3x3和1x1的滤波器，而且包含像ResNet中的残差连接。Darknet-53相较于ResNet-152有更少的浮点运算数，但是可以以两倍的速度达到同样的分类准确率。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*biRYJyCSv-UTbTQTa4Afqg.png)\n",
    "\n",
    "### YOLOv3性能\n",
    "YOLOv3在COCO上的AP指标媲美SSD但是要快3倍。但是YOLOv3的AP仍然落后于RetinaNet。特别的，AP@IoU=.75相较于RetinaNet差的很多因为YOLOv3有更好的定位误差。YOLOv3在较小的物体上表现了极大的提升。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/1000/1*bFMwN__ZgfvVRYuo4o8EEg.png)\n",
    "YOLOv3在高速度的检测器中表现得很高。<br>\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*RFpjH8D6TStBaYuZYehe_g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 资源\n",
    "- [YOLO原始论文](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "- [YOLOv2和YOLO9000](https://arxiv.org/pdf/1612.08242.pdf)\n",
    "- [DarkNet实现](https://github.com/pjreddie/darknet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
