{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用PyTorch解决几乎所有的图像分类问题\n",
    "![avater](https://cdn-images-1.medium.com/max/800/1*jcZLpgh3gppeFFgcpFSP0w.jpeg)\n",
    "这是一个基于PyTorch构建代码的实验。主要的目的在于快速地应用预训练模型做迁移学习。我们在本博文中将使用植物苗分类数据集。这是Kaggle上的一个[比赛](https://www.kaggle.com/c/plant-seedlings-classification)。\n",
    "下面的预训练模型是PyTorch可用的模型\n",
    "- resnet18,resnet34,resnet50,resnet101,resnet152\n",
    "- squeezenet1_0,squeezenet1_1\n",
    "- Alexnet\n",
    "- inception_v3\n",
    "- Densenet121,Densenet169,Densenet201\n",
    "- Vgg11,vgg13,vgg16,vgg19,vgg11_bn,vgg13_bn,vgg16_bn,vgg19_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迁移学习的三种情况以如何使用PyTorch解决\n",
    "我已经在之前的[文章](https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8)中讨论过迁移学习的原理了。这里稍微提一下。<br>\n",
    "1. 冻结除了最后一层之外的所有层\n",
    "2. 冻结前面的几层\n",
    "3. 在整个网络上微调\n",
    "如果你知道了模型的结构，那么在PyTorch中可以很直接的进行迁移学习。所有上面提到的模型实现都是不同的。有些是使用序列化的容器，包含很多层，有些就直接是一些层。所以需要仔细PyTorch中模型的定义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet和Inception_v3\n",
    "有好几种Resnet的实现，我们可以按需选择使用。由于Imagenet数据集有1000个类别，我们需要根据我们的需要改变最后一层的输出。我们需要冻结所以不需要训练的层并且将需要训练的参数传向优化器。<br>\n",
    "```python\n",
    "if resnet:\n",
    "    model_conv = torchvision.models.resnet50()\n",
    "\n",
    "if inception:\n",
    "    model_conv = torchvision.models.inception_v3()\n",
    "    \n",
    "## Change the last layer\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, n_class)\n",
    "```\n",
    "model_conv是一个容器，每个孩子又有各自的孩子（层）。下面是resnet50的例子。<br>\n",
    "```python\n",
    "for name, child in model_conv.named_children():\n",
    "    for name2, params in child.named_parameters():\n",
    "        print(name, name2)\n",
    "## A long list of param are listed, some of them are shown below,\n",
    "conv1 weight\n",
    "bn1 weight\n",
    "bn1 bias\n",
    "....\n",
    "fc weight\n",
    "fc bias\n",
    "```\n",
    "现在我们希望在训练前冻结一些层，我们可以简单的使用如下命令：<br>\n",
    "```python\n",
    "## Freezing all layers\n",
    "for params in model_conv.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "## Freezing the first few layers. Here I am freezing the first 7 layers\n",
    "ct = 0\n",
    "for name, child in model_conv.named_children():\n",
    "    ct += 1\n",
    "    if ct < 7:\n",
    "        for name2, params in child.named_parameters():\n",
    "            params.requires_grad = False\n",
    "```\n",
    "改变最后一层以适应新的数据是需要技巧的，我们需要仔细检查层。我们已经看了ResNet和Inception_V3。现在来看看其他网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeeze-Net\n",
    "在PyTorch中有两类squeeze-net，我们可以任意选用。不同于resnet最后有一个fc层，Squeeze-net最后一层是一个包装起来的容器（序列的），所以我们需要将所有的子层列出来，然后对需要的层做转换，然后放到容器中。详见如下代码。<br>\n",
    "```python\n",
    "model_conv = torchvision.models.squeezenet1_1()\n",
    "for name, params in model_conv.named_children():\n",
    "    print(name)\n",
    "\n",
    "'''\n",
    "features\n",
    "classifier\n",
    "'''\n",
    "## How many In_channels are there for the conv layer\n",
    "in_ftrs = model_conv.classifier[1].in_channels\n",
    "\n",
    "## How many Out_channels are there for the conv layer\n",
    "out_ftrs = model_conv.classifier[1].out_channels\n",
    "\n",
    "## Converting a sequential layer to list of layers\n",
    "features = list(model_conv.classifier.children())\n",
    "\n",
    "## Changing the conv layer to required dimension\n",
    "features[1] = nn.Conv2d(in_ftrs, n_class, kernel_size, stride)\n",
    "\n",
    "## Changing the pooling layer as per the architecture output\n",
    "features[3] = nn.AvgPool2d(12, stride=1)\n",
    "\n",
    "## Making a container to list all the layers\n",
    "model_conv.classifier = nn.Sequential(*features)\n",
    "\n",
    "## Mentioning the number of out_put classes\n",
    "model_conv.num_classes = n_class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense-Net\n",
    "它的结构跟Resnet类似但是最后一层的名字是classifier，代码如下：<br>\n",
    "```python\n",
    "model_conv = torchvision.models.densenet121(pretrained='imagenet')\n",
    "num_ftrs = model_conv.classifier.in_features\n",
    "model_conv.classifier = nn.Linear(num_ftrs,n_class)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG以及Alex-Net\n",
    "跟Squeeze-net类似。最后的fc层包装在一个容器中，所以我么你需要读取到容器然后改变其最后的fc层。<br>\n",
    "```python\n",
    "modle_conv = torchvision.models.vgg19(pretrained='imagenet')\n",
    "# Number of filters in the bottleneck layer\n",
    "num_ftrs = model_conv.classifier[6].in_features\n",
    "\n",
    "# convert all the layers to list and remove the last one\n",
    "features = list(model_conv.classifier.children())[:-1]\n",
    "\n",
    "## Add the last layer based on the num of classes in our dataset\n",
    "features.extend([nn.Linear(num_ftrs, n_class)])\n",
    "\n",
    "## convert it into container and add it to our model class\n",
    "model_conv.classifier = nn.Sequential(*features)\n",
    "```\n",
    "我们可以学到如何冻结需要的层以及改变不同网络的最后层。现在我们来使用其中一个训练网络。这里只是简述，详见我的Github。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base code\n",
    "像所有深度学习模型一样，我们首先<br>\n",
    "- 定义一个网络\n",
    "- 加载可用的预训练权重\n",
    "- 冻结不需要训练的层（冻结的层就可以视为特征提取器）\n",
    "- 加上损失\n",
    "- 选择优化器\n",
    "- 训练网络直到达标\n",
    "现在我们以inception_v3为例来看看。我们将冻结前几层然后用SGD优化器和动量以及交叉熵损失来训练网络。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "## Load the model \n",
    "model_conv = torchvision.models.inception_v3(pretrained='imagenet')\n",
    "\n",
    "## Lets freeze the first few layers. This is done in two stages \n",
    "# Stage-1 Freezing all the layers \n",
    "if freeze_layers:\n",
    "  for i, param in model_conv.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Since imagenet as 1000 classes , We need to change our last layer according to the number of classes we have,\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, n_class)\n",
    "\n",
    "# Stage-2 , Freeze all the layers till \"Conv2d_4a_3*3\"\n",
    "ct = []\n",
    "for name, child in model_conv.named_children():\n",
    "    if \"Conv2d_4a_3x3\" in ct:\n",
    "        for params in child.parameters():\n",
    "            params.requires_grad = True\n",
    "    ct.append(name)\n",
    "    \n",
    "# To view which layers are freeze and which layers are not freezed:\n",
    "for name, child in model_conv.named_childeren():\n",
    "  for name_2, params in child.named_parameters():\n",
    "    print(name_2, params.requires_grad)\n",
    "    \n",
    "## Loading the dataloaders -- Make sure that the data is saved in following way\n",
    "\"\"\"\n",
    "data/\n",
    "  - train/\n",
    "      - class_1 folder/\n",
    "          - img1.png\n",
    "          - img2.png\n",
    "      - class_2 folder/\n",
    "      .....\n",
    "      - class_n folder/\n",
    "  - val/\n",
    "      - class_1 folder/\n",
    "      - class_2 folder/\n",
    "      ......\n",
    "      - class_n folder/\n",
    "\"\"\"\n",
    "\n",
    "data_dir = \"data/\"\n",
    "input_shape = 299\n",
    "batch_size = 32\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "scale = 360\n",
    "input_shape = 299 \n",
    "use_parallel = False\n",
    "use_gpu = True\n",
    "epochs = 100\n",
    "\n",
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "        transforms.Resize(scale),\n",
    "        transforms.RandomResizedCrop(input_shape),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(degrees=90),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)]),\n",
    "        'val': transforms.Compose([\n",
    "        transforms.Resize(scale),\n",
    "        transforms.CenterCrop(input_shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)]),}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                      data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "if use_parallel:\n",
    "    print(\"[Using all the available GPUs]\")\n",
    "    model_conv = nn.DataParallel(model_conv, device_ids=[0, 1])\n",
    "\n",
    "print(\"[Using CrossEntropyLoss...]\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"[Using small learning rate with momentum...]\")\n",
    "optimizer_conv = optim.SGD(list(filter(lambda p: p.requires_grad, model_conv.parameters())), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"[Creating Learning rate scheduler...]\")\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "print(\"[Training the model begun ....]\")\n",
    "# train_model function is here: https://github.com/Prakashvanapalli/pytorch_classifiers/blob/master/tars/tars_training.py\n",
    "model_ft = train_model(model_conv, dataloaders, dataset_sizes, criterion, optimizer_conv, exp_lr_scheduler, use_gpu,\n",
    "                     num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "https://www.kaggle.com/c/plant-seedlings-classification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指标\n",
    "我又进一步研究了这些模型在不同设置下的表现。我的思路是所有网络单独训练，然后最后应用几种不同的集成方法来提升准确率。我还比较关心不同模型的差异化，所以下面是训练和验证集上的指标。<br>\n",
    "### Update-1\n",
    "Cadene训练了一些Pytorch官方没有的模型，我使用了他的部分代码训练了以下模型。<br>\n",
    "- resnext101_64x4d\n",
    "- resnext101_32x4d\n",
    "- nasnetalarge\n",
    "- inceptionresnetv2\n",
    "- inceptionv4\n",
    "我在bn_inception和vggm上遇到了一些问题，将会在之后更新。\n",
    "### TODO:\n",
    "1. 为所有网络加入了混合策略\n",
    "2. 集成模型的输出\n",
    "3. 模型堆叠\n",
    "4. 提取瓶颈特征然后训练\n",
    "5. 可视化T-sne\n",
    "6. 解决bn_inception的问题（模型不训练）\n",
    "7. 训练Vggm\n",
    "8. SE-Net实现和训练\n",
    "### 最终提交结果\n",
    "提交时是Leaderboard的第29名。\n",
    "### Github Repo\n",
    "- 代码：https://github.com/Prakashvanapalli/pytorch_classifiers.\n",
    "- 我可以分享一些预训练权重和预测文件来构建不同的集成模型。给我发邮件即可**vanapaliprakash@gmail.com**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
